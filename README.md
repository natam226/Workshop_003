# Workshop 003: Happiness Score Prediction with Machine Learning and Kafka

This project aims to train a regression model to predict the happiness score of different countries using historical data, with data streamed via Kafka. The pipeline includes exploratory data analysis (EDA), feature engineering, model training, real-time data streaming, prediction, and storing the results in a database.


## ğŸ§ª Technologies Used

- Python
- Jupyter Notebook
- Apache Kafka
- Scikit-learn
- PostgreSQL


## ğŸ“ Structure
```
.
â”œâ”€â”€ data/                    # CSV files with happiness data by year
â”œâ”€â”€ models/                  # Trained model (.pkl file)
â”œâ”€â”€ notebooks/               # Jupyter notebooks for EDA, modeling, and prediction
â”œâ”€â”€ streaming/               # Kafka producer and consumer scripts
â”œâ”€â”€ database/                # Scripts to insert predictions into the database
â””â”€â”€ README.md
```


## ğŸ” Exploratory Data Analysis (EDA)

We analyzed 5 CSV files from different years. Common features were identified and inconsistencies across datasets were handled to ensure uniformity in feature selection.


## âš™ï¸ ETL and Modeling Process

- Load and clean data.

- Select consistent features across all datasets.

- Split data into 80% training and 20% testing.

- Train a regression model using scikit-learn.

- Serialize the trained model to a .pkl file.


## ğŸ”„ Data Streaming with Kafka

- **_Kafka Producer_**: Streams transformed feature data.

- **_Kafka Consumer_**: Receives the data, uses the trained model to predict the happiness score, and stores the result in a database.

The following information is stored:

- Features used for the prediction
- Testing data
- Predicted data (happiness score)


## ğŸ“Š Performance Metric

A regression performance metric (RÂ²) was used to evaluate the model on the testing dataset. The best performing model was **_CatBoost Regressor_**, achieving an **_RÂ²_** score of **_0.86_**.


## ğŸš€ How to Run

### Installing the dependencies
The necessary dependencies are stored in a file named requirements.txt. To install the dependencies you can use the command
```bash
pip install -r requirements.txt
```

### Initialize kafka

Open a terminal in Visual Studio Code and start docker
```bash
docker-compose up -d --build
```

Run this command in a terminal to initialize the consumer
```bash
python3 kafka_consumer.py 
```

Open a new terminal and run this command to initialize the producer
```bash
python3 feature_selection.py
```



